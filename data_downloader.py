import asyncio
from os.path import join
import traceback
from typing import Coroutine, Dict

import aiofiles
import aiohttp
from bs4 import BeautifulSoup

import tools
from logger import create_logger

logger = create_logger('logs/vk_parser.log', 'data_downloader', 'DEBUG')


def get_response_info(content_type: str) -> Dict[str, str]:
    '''
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ `content-type`
    `content_type`: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ø—Ä–æ—Å–∞

    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∞:
    ```
    {
        'encoding': '–ö–æ–¥–∏—Ä–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –µ—Å–ª–∏ –æ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π, –∏–Ω–∞—á–µ –∫–ª—é—á –±—É–¥–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å',
        'data_type': '–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, text –∏–ª–∏ image',
        'extension': '–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, html –∏–ª–∏ gif',
        'full_type_info': '–ö–æ–º–±–∏–Ω–∞—Ü–∏—è data_type –∏ extension —á–µ—Ä–µ–∑ /'
    }
    ```
    '''
    result = {}
    find_res = content_type.find('charset=')
    if find_res != -1:
        result.update({'encoding': content_type[find_res + len('charset='):]})
    find_res = content_type.find(';')
    if find_res != -1:
        full_type_info = content_type[:find_res]
        data_type, extension = full_type_info.split('/')
    else:
        full_type_info = content_type
        data_type, extension = full_type_info.split('/')
    result.update({
        'data_type': data_type,
        'extension': extension,
        'full_type_info': full_type_info
    })
    return result


async def find_link_by_url(session: aiohttp.ClientSession, url: str) -> str:
    '''
    –ù–∞—Ö–æ–¥–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç VK
    `session`: —Å–µ—Å—Å–∏—è
    `url`: —Å—Å—ã–ª–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç VK, –≥–¥–µ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É
    '''
    async with session.get(url, timeout=5) as response:
        assert response.status == 200, f'Response status: {response.status}'
        soup = BeautifulSoup(await response.text(), 'html.parser')
        assert '–û—à–∏–±–∫–∞' not in soup.find('title').text, '–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É'
        return soup.find('img').get('src')


async def downloader(response: aiohttp.ClientResponse, path: str, name: str) -> Coroutine:
    '''
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –∏–∑ `response`:
    `response`: –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å
    `path`: –ø—É—Ç—å, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª
    `name`: –∏–º—è —Å–æ—Ö—Ä–∞–Ω—è–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
    '''
    async with aiofiles.open(join(path, name), 'wb') as f:
        async for data in response.content.iter_any():
            await f.write(data)


async def get_info(url: str, save_path: str, file_name: str, sema: asyncio.BoundedSemaphore) -> Dict[str, str] | None:
    '''
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –∏–∑ `response`, –≤–æ–∑–≤—Ä–∞—â–∞—è –æ –Ω–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:
    ```
    {
        'url': URL —Å–∫–∞—á–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞,
        'file_info': –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–µ —Ñ–∞–π–ª–∞
    }
    ```

    `url`: —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª –∏–ª–∏ —Ä–µ—Å—É—Ä—Å
    `save_path`: –ø—É—Ç—å –¥–æ –ø–∞–ø–∫–∏, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª
    `file_name`: –∏–º—è —Ñ–∞–π–ª–∞
    `sema`: —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    '''
    try:
        async with sema, aiohttp.ClientSession(headers={'Accept-Language': 'ru'}) as session:
            async with session.get(url, timeout=15) as response:
                assert response.status == 200, f'Response status: {response.status}'
                if any(t in response.headers['content-type'] for t in ('image', 'audio')):
                    response_info = get_response_info(response.headers['content-type'])
                    download_path = join(save_path, response_info['full_type_info'])
                    tools.create_folder(download_path)
                    await asyncio.create_task(
                        downloader(
                            response=response,
                            path=download_path,
                            name=f'{file_name}.{response_info["extension"]}'
                        )
                    )
                    return {'url': response.url, 'file_info': response_info['full_type_info']}
                target_content_type = response.headers['content-type']
            if 'text/html' in target_content_type and 'vk.com/doc' in url:
                link = await asyncio.create_task(find_link_by_url(session, response.url))
                async with session.get(link, timeout=15) as response:
                    response_info = get_response_info(response.headers['content-type'])
                    download_path = join(save_path, response_info['full_type_info'])
                    tools.create_folder(download_path)
                    await asyncio.create_task(
                        downloader(
                            response=response,
                            path=download_path,
                            name=f'{file_name}.{response_info["extension"]}'
                        )
                    )
                    return {'url': response.url, 'file_info': response_info['full_type_info']}
            return {'url': response.url, 'file_info': 'not_parse'}
    except Exception as e:
        logger.error(f'–û—à–∏–±–∫–∞ üîó {url}: {e}')
        logger.debug(traceback.format_exc())
        return {'url': url, 'file_info': 'error'}
