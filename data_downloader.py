import asyncio
import ssl
from configparser import ConfigParser
from os.path import join
from traceback import format_exc
from typing import Coroutine, Dict

import aiofiles
import aiohttp
from bs4 import BeautifulSoup
from latest_user_agents import get_random_user_agent

import tools
from logger import create_logger

config = ConfigParser()
config_read = config.read('config.ini', encoding='utf8')
if config_read is None:
    logger = create_logger('logs/vk_parser.log', 'data_downloader', 'DEBUG')
else:
    log_level = config['main_parameters'].get('log_level', 'DEBUG')
    logger = create_logger('logs/vk_parser.log', 'data_downloader', log_level)

error_titles = [
    '–û—à–∏–±–∫–∞ | –í–ö–æ–Ω—Ç–∞–∫—Ç–µ',
    '–û—à–∏–±–∫–∞',
    'Error | VK',
    'Error'
]

error_titles_html = [f'<title>{x}</title>' for x in error_titles]


def get_response_info(content_type: str) -> Dict[str, str]:
    '''
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ `content-type`
    `content_type`: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ø—Ä–æ—Å–∞

    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∞:
    ```
    {
        'encoding': '–ö–æ–¥–∏—Ä–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –µ—Å–ª–∏ –æ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π, –∏–Ω–∞—á–µ –∫–ª—é—á –±—É–¥–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å',
        'data_type': '–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, text –∏–ª–∏ image',
        'extension': '–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, html –∏–ª–∏ gif',
        'full_type_info': '–ö–æ–º–±–∏–Ω–∞—Ü–∏—è data_type –∏ extension —á–µ—Ä–µ–∑ /'
    }
    ```
    '''
    result = {}
    find_res = content_type.find('charset=')
    if find_res != -1:
        result.update({'encoding': content_type[find_res + len('charset='):]})
    find_res = content_type.find(';')
    if find_res != -1:
        full_type_info = content_type[:find_res]
        data_type, extension = full_type_info.split('/')
    else:
        full_type_info = content_type
        data_type, extension = full_type_info.split('/')
    result.update({
        'data_type': data_type,
        'extension': extension,
        'full_type_info': full_type_info
    })
    return result


def get_file_name_by_link(link: str) -> str | None:
    try:
        if '?extra=' in link:
            return link.split('/')[-1].split('?extra=')[0]
        elif '?size=' in link:
            return link.split('/')[-1].split('?size=')[0]
        else:
            return link.split('/')[-1]
    except Exception:
        return None


def check_vk_title_error(soup: BeautifulSoup) -> str | bool:
    '''
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–∫–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É VK
    `soup`: —ç–∫–∑–µ–º–ø–ª—è—Ä `BeautifulSoup` –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å `HTML` –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - `True`: –û—à–∏–±–∫–∏ –Ω–µ—Ç
    - `False`: –û—à–∏–±–∫–∏ –µ—Å—Ç—å (–æ—à–∏–±–∫–∏ –¥–æ—Å—Ç—É–ø–∞, –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏, —Å–∫—Ä—ã—Ç–∏—è)
    '''
    mes = soup.find('div', class_='message_page_title')
    if mes is not None:
        if any(ext in mes.text for ext in error_titles):
            details = soup.find('div', class_='message_page_body').text.strip().split('\n')[0]
            return details
    return False


async def find_link_by_url(session: aiohttp.ClientSession, url: str, pattern: str, cookies=None) -> str:
    '''
    –ù–∞—Ö–æ–¥–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ VK. –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π `url`
    `session`: —Å–µ—Å—Å–∏—è
    `url`: —Å—Å—ã–ª–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç VK, –≥–¥–µ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É
    `pattern`: –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞
    - `doc`: –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    `cookies` –∫—É–∫–∏ –¥–ª—è `aiohttp.ClientResponse`
    '''
    async with session.get(url, timeout=60, cookies=cookies) as response:
        if 'doc' in pattern:
            doc_url_pattern = 'docUrl":"'
            doc_buy_pattern = '","docBuyLink'
            assert response.status == 200, f'Response status: {response.status}'
            if 'text/html' in response.headers['content-type']:
                text = await response.text()
                assert not any(ext in text for ext in error_titles_html), '–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É'
                first = text.find(doc_url_pattern)
                second = text.find(doc_buy_pattern)
                return text[first + len(doc_url_pattern):second].replace('\/', '/')
        elif 'photo' in pattern:
            soup = BeautifulSoup(await response.text(), 'html.parser')
            check = check_vk_title_error(soup)
            assert not check, f'–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–æ—Ç–æ: {check}'
            link = soup.find('meta', attrs={'name': 'og:image:secure_url'})
            if link:
                return link['value']
        redirect_url = str(response.url)
        if redirect_url != url:
            return redirect_url
        return url


async def downloader(response: aiohttp.ClientResponse, path: str, name: str) -> Coroutine:
    '''
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –∏–∑ `response`:
    `response`: –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å
    `path`: –ø—É—Ç—å, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª
    `name`: –∏–º—è —Å–æ—Ö—Ä–∞–Ω—è–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
    '''
    block_size = 16384
    async with aiofiles.open(join(path, name), 'wb') as f:
        while True:
            data = await response.content.read(block_size)
            if not data:
                break
            await f.write(data)


async def get_info(
    url: str,
    save_path: str,
    file_name: str,
    session: aiohttp.ClientSession,
    semaphore: asyncio.BoundedSemaphore,
    cookies=None
) -> Dict[str, str] | None:
    '''
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –∏–∑ `response`, –≤–æ–∑–≤—Ä–∞—â–∞—è –æ –Ω–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:
    ```
    {
        'url': URL —Å–∫–∞—á–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞,
        'file_info': –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–µ —Ñ–∞–π–ª–∞
    }
    ```

    `url`: —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª –∏–ª–∏ —Ä–µ—Å—É—Ä—Å
    `save_path`: –ø—É—Ç—å –¥–æ –ø–∞–ø–∫–∏, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª
    `file_name`: –∏–º—è —Ñ–∞–π–ª–∞
    `sema`: —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    `cookies` –∫—É–∫–∏ –¥–ª—è `aiohttp.ClientSession`
    '''
    try:
        if 'vk.com/video' in url:
            return {'url': url, 'file_info': 'vk_video'}
        elif 'vk.com/id' in url or 'vk.com/public' in url:
            return {'url': url, 'file_info': 'vk_contact'}
        elif 'vk.com/story' in url:
            return {'url': url, 'file_info': 'vk_story'}
        elif 'github.com' in url:
            return {'url': url, 'file_info': 'github_link'}
        elif 'aliexpress.com' in url:
            return {'url': url, 'file_info': 'aliexpress_link'}
        elif 'pastebin.com' in url:
            return {'url': url, 'file_info': 'pastebin_link'}
        elif 'drive.google.com' in url:
            return {'url': url, 'file_info': 'gdrive_link'}
        elif 'google.com' in url:
            return {'url': url, 'file_info': 'google_link'}
        elif 'wikipedia.org' in url:
            return {'url': url, 'file_info': 'wikipedia_link'}
        elif 'pornhub.com' in url:
            return {'url': url, 'file_info': 'üçì'}
        elif 't.me' in url:
            return {'url': url, 'file_info': 'telegram_contact'}
        elif 'dns-shop.ru' in url:
            return {'url': url, 'file_info': 'dns_shop_link'}
        elif 'habr.com' in url:
            return {'url': url, 'file_info': 'habr_link'}

        async with semaphore:
            async with session.get(url, timeout=45) as response:
                assert response.status == 200, f'Response status: {response.status}'
                if any(t in response.headers['content-type'] for t in ('image', 'audio')):
                    response_info = get_response_info(response.headers['content-type'])
                    download_path = join(save_path, response_info['full_type_info'])
                    tools.create_folder(download_path)
                    download_file_name = get_file_name_by_link(str(response.url))
                    if download_file_name is None:
                        download_file_name = f'{file_name}.{response_info["extension"]}'
                    await asyncio.create_task(
                        downloader(
                            response=response,
                            path=download_path,
                            name=download_file_name
                        )
                    )
                    return {'url': str(response.url), 'file_info': response_info['full_type_info']}
                target_content_type = response.headers['content-type']

                if 'text/html' in target_content_type:
                    if 'vk.com/doc' in url:
                        find_res = await asyncio.create_task(find_link_by_url(session, url, 'doc', cookies))
                    elif 'vk.com/photo':
                        find_res = await asyncio.create_task(find_link_by_url(session, url, 'photo', cookies))
                    if find_res == url:
                        return {'url': find_res, 'file_info': 'not_parse'}
                    async with session.get(find_res, timeout=900) as response:
                        response_info = get_response_info(response.headers['content-type'])
                        download_path = join(save_path, response_info['full_type_info'])
                        tools.create_folder(download_path)
                        download_file_name = get_file_name_by_link(find_res)
                        if download_file_name is None:
                            download_file_name = f'{file_name}.{response_info["extension"]}'
                        await asyncio.create_task(
                            downloader(
                                response=response,
                                path=download_path,
                                name=download_file_name
                            )
                        )
                        return {'url': str(response.url), 'file_info': response_info['full_type_info']}

            return {'url': url, 'file_info': 'not_parse'}
    except asyncio.TimeoutError as e:
        logger.error(f'–û—à–∏–±–∫–∞ üîó {url}: —Ç–∞–π–º-–∞—É—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏—è')
        logger.debug(format_exc())
        return {'url': url, 'file_info': 'timeout_error'}
    except Exception as e:
        logger.error(f'–û—à–∏–±–∫–∞ üîó {url}: {e}')
        logger.debug(format_exc())
        return {'url': url, 'file_info': 'error'}
